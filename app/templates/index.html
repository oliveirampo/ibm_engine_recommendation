<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Recommendation Engine</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/4.0/examples/blog/">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<!--    <link rel="stylesheet" href="static/styles/css/bootstrap.min.css">-->

    <!--plotly    -->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

    <!-- Custom styles for this template -->
    <link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
    <link href="static/styles/css/blog.css" rel="stylesheet">
    <link href="static/styles/css/extra.css" rel="stylesheet">

</head>
<body>

<div class="container">
    <header class="blog-header py-3">
        <div class="row flex-auto text-justify align-center">
            <div class="col-12 d-flex justify-content-end align-items-center">

                <a href="https://github.com/oliveirampo/ibm_engine_recommendation" target=”_blank”>
                    <svg width="24" height="24" viewBox="0 0 16 16">
                        <path fill="#828282"
                            d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z">
                        </path>
                    </svg>
                </a>

                &nbsp

                <a href="https://www.linkedin.com/in/marina-pereira-oliveira/" target=”_blank”>
                    <span class="icon icon--github">
                        <svg width="24" height="24" viewBox="0 0 24 24">
                            <path fill="#828282"
                                d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z">
                            </path>
                        </svg>
                    </span>
                </a>

            </div>
        </div>
    </header>

<!--    <div class="nav-scroller py-1 mb-2">-->
<!--        <nav class="nav d-flex justify-content-between">-->
<!--            <a class="p-2 text-muted" href="#">Knowledge-based RS</a>-->
<!--            <a class="p-2 text-muted" href="#">Collaborative filtering RS</a>-->
<!--            <a class="p-2 text-muted" href="#">Content-based RS</a>-->
<!--            <a class="p-2 text-muted" href="#">Data</a>-->
<!--        </nav>-->
<!--    </div>-->

    <div class="jumbotron p-3 p-md-5 text-white rounded bg-dark">
        <div class="col-md-6 px-0">
            <h1 class="display-4 font-italic">Recommendation Engine with IBM</h1>
            <p class="lead my-3">
                Making recommendations using a dataset
                with interactions that users have with articles on the IBM Watson Studio.
            </p>
        </div>
    </div>

    <div class="blog-post">
        <h2 class="blog-post-title">1. Exploring and Cleaning the Data</h2>

        <p>
            In this first part, we will take a look at the data,
            present the most relevant information,
            along with the cleaning steps for further analysis.
            The Jupyter notebook used to process this data can be found
            <a href="https://github.com/oliveirampo/ibm_engine_recommendation/blob/main/scr/recommendations_with_IBM.ipynb" target="_blank">here</a>.
        </p>

        <hr>
        <p>Two data sets were used in this study:</p>

        {% block content %}
        {% endblock %}

        <ul>
          <li><strong>df</strong>: maps users to articles.</li>

            {{ df.head(5).to_html(notebook=True, table_id="mytable") | safe }}

          <li><strong>df_content</strong>: contains information about articles.</li>

            {{ df_content.head(5).to_html(notebook=True, table_id="mytable") | safe }}
        </ul>

        <h4>General Information</h4>

        <p>
            There are
            714 unique articles that have at least one interaction,
            from a total of 1051 unique articles on the IBM platform,
            5148 unique users,
            and
            45993 user-article interactions.
        </p>

        <div>
            <img class="marginauto"
                alt="number_of_users_and_articles"
                height="auto"
                src="{{ url_for('static', filename='/images/numbers_user_articles.png') }}">
        </div>

        <div class="parent center">
            <div class="image1 col-md-12" id="histogram_articles_user"></div>
            <img class="image2" src="{{ url_for('static', filename='images/infographic_interactions.png') }}"
                alt="number_of_users_and_articles"
                width="400px"/>
        </div>

        &nbsp

        <h4>Most Frequent Key-Words in the Articles</h4>

        &nbsp

        <div>
            <img src="{{ url_for('static', filename='images/word_cloud_titles.png') }}"
                alt="word_cloud_titles"
                class="marginauto"/>
        </div>


        <h4>Data Cleaning</h4>

        <p>
            As you noticed, the e-mails in the <strong>df</strong> dataframe
            are encrypted in order to preserve anonymity.
            The content of the strings is irrelevant, and for practical reasons,
            they were mapped to unique identifiers with integer format.
            In addition to that, duplicate rows were eliminated.
        </p>

        <p>Thus, the following steps were taken for cleaning the data:</p>

        <ol>
            <li>Replace e-mails by indexes.</li>
            <li>Remove any rows that have the same article id, and only keep the first entry.</li>
        </ol>

        <h2>2. Rank-Based Recommendations</h2>

        <p>
            When building a recommendation system, the most straightforward step
            is to build a non-personalized recommendation.
            It is easy and does not require specific knowledge about the users.
            This is useful not only to provide a general picture of the data,
            but also to provide initial recommendations to new users.
        </p>

        <hr>

        <h4>Top 10 Articles</h4>

        <p>
            In our case, we ranked the articles based on their popularity.
            But note that we don't actually have ratings for whether a user liked an article or not.
            We only know that a user has interacted with an article.
            Thus, the popularity of an article was based on how often an article was interacted with.
            The table below show the 10 most interacted with articles.
        </p>


        <table id="mytable">
            <tr>
                <th>Top 10 Articles</th>
            </tr>
            {% for article in top_10_articles %}
            <tr>
            <!--<TD class="c1"><IMG src="favicon.ico"></TD>-->
               <td>{{article.capitalize()}}</td>
            </tr>
            {% endfor %}
        </table>

        &nbsp

        <h4>Top 10 Articles by Topic</h4>

        <div id="collapsibleTree">
          <div class="observablehq-chart" ></div>
        </div>
        <div>
            <a style="font: 0.8em sans-serif;border: 1px solid #efefef;padding: 0.4em;border-radius: 1em;display: inline-flex;align-items: center;color: inherit;text-decoration: none;"
           href="https://observablehq.com/@d3/collapsible-tree" target="_blank" rel="noopener noreferrer"><svg role="img" viewBox="0 0 25 28" width="16" height="16" aria-label="Observable" fill="currentColor"><path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z" fill="currentColor"></path></svg><strong style="margin: 0 4px;">D3</strong> Collapsible Tree</a>
        </div>

        &nbsp

        <h2>3. User-User Based Collaborative Filtering</h2>

        <p>
            Another way to recommend articles to a user is by
            making use of the similarity between users.
            We only have to obtain the most similar users to a given user,
            and recommend the new articles from the formers to the later.
        </p>

        <hr>

        <p>
            The following steps were taken:
        </p>

        <ol>
            <li>Reformat <strong>df</strong> dataframe to more easily compute similarity.</li>
            <li>Obtain an ordered list of the most similar user to a given user.</li>
            <li> Use this list o similar users to find articles to be recommended.</li>
        </ol>

        <h4>Reformatting the Data</h4>

        <p>
            We did that by reformatting the <strong>df</strong> dataframe
            such that the rows refer to users and the columns to article ids.
            It is important to mention that
            we neglected how many time a user interacted with the article,
            thus, all entries where a user interacted with an article was a 1,
            and in the opposite case a 0.
        </p>

        <p>
            {{ user_item.head(5).to_html(notebook=True, table_id="mytable") | safe }}
        </p>


        <h4>List of Similar Users</h4>

        <p>
            The results for each user in the above table are binary, that is why
            it makes sense to compute similarity as the dot product of two users.
        </p>

        <p>
            The figure below shows the most similar users for the first 100 users.
            When you hover over the user id,
            the <span style="color:red;">red lines</span> show the closest neighbor to the given user,
            and the  <span style="color:#2f4f4f;">green lines</span> show that the given user is one of the most similar users
            of the connected users.
        </p>

        <div id="edgeBundling">
          <div class="observablehq-chart" ></div>
        </div>
        <div>
            <a style="font: 0.8em sans-serif;border: 1px solid #efefef;padding: 0.4em;border-radius: 1em;display: inline-flex;align-items: center;color: inherit;text-decoration: none;"
           href="https://observablehq.com/@d3/hierarchical-edge-bundling" target="_blank" rel="noopener noreferrer">
                <svg role="img" viewBox="0 0 25 28" width="16" height="16" aria-label="Observable" fill="currentColor">
                    <path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z" fill="currentColor"></path>
                </svg><strong style="margin: 0 4px;">D3</strong> Hierarchical Edge Bundling</a>
        </div>

        &nbsp

        <h4>Recommend Articles</h4>

        <p>
            Once we know the most similar users to a given user,
            we can recommend their new articles to this user.
            For users with same similarity we chose first:
        </p>

        <ul>
            <li>users that have the most total article interactions,</li>
            <li>and articles with most interactions.</li>
        </ul>


        <h2>4. Matrix Factorization</h2>

        <p>
            Another way to make recommendations consists of using matrix factorization
            with the Singular Value Decomposition (SVD) method.
            The basic idea is that by looking at the behavioral data of the users,
            one can find hidden categories that can be used to explain user's taste.
            These hidden trends in the data are called latent features.
            They are also latent because even if they make sense data-wise,
            it might not be so easy to say what these factors mean.
            For more information see
            <a href="https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/">
                Practical Recommender Systems book.</a>
        </p>

        <p>
            A matrix can be factorized into three matrices
            (Ref.:
            <a href="https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/">
                Practical Recommender Systems
            </a>
            ):
        </p>

        <figure>
            <img class="center"
             src="static/images/matrix_factorization.png" alt="svd_accuracy">
        </figure>

        <ul>
            <li>M - A martix you want to decompose.</li>
            <li>U - User feature matrix.</li>
            <li>&Sigma; - Weights diagonal.</li>
            <li>V<sup>T</sup> - Item feature matrix.</li>
        </ul>

        <p>
            The idea is that the central diagonal matrix &Sigma; contains elements
            (called singular values)
            that are sorted from the largest to the smallest.
            They indicate how much information these features produce for the data set.
            A feature here means both a column in the user matrix U
            and a row in the content matrix V<sup>T</sup>.
            After decomposition, one can select only the features with highest score,
            thus the size of the matrices can be reduced.
            See
            <a href="https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/">
                Practical Recommender Systems
            </a>
            , for instance, for more details.
        </p>

        <p>
            The advantage of SVD is that it is possible to add new users easily.
            However this method is slow to calculate, and when the dataset is large
            it will be time-consuming.
            Another issue is that the entries cannot have missing values.
            As solutions to this problem one can choose one the following options:

            <ul>
                <li>Calculate the mean of each item (or user)
                    and fill this mean value where there are missing values; </li>
                <li>Normalize each row, such that all elements are centered around zero,
                    so the zeros wil become the average;</li>
                <li>Extract the biases for the items and users,
                    and provide a baseline for the predictions.</li>

            </ul>
        </p>

        For more details see
        <a href="https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/">
            Practical Recommender Systems
        </a>.

        <p>
            One key decision when using SVD is the number of latent features to retain.
            One way to define that is to divide the data into training and test sets,
            and selected the number of features with highest accuracy for both sets.
            The accuracy of the train and test sets as a function of
            the number of features for the dataset used in this project is shown above.
        </p>

        <img class="center"
             src="static/images/svd_accuracy.png" alt="svd_accuracy">

        <p>
            The results above show that
            the higher the number of features,
            the higher the accuracy in the train set
            and the lower the accuracy in the test set.
            And if the decision of the number of features is entirely based on the results
            for the test set, then the fewer the better.
            However, it is worth mentioning that the size of the test set is too small
            (20 users only),
            and the &Sigma; matrix should still retain about 90% of the information.
            Thus such decision must be taken carefully, considering both aspects.
        </p>

        <p>
            With the factorization in place,
            we can look it up in the new users-items matrix to make predictions,
            making use of the reduced matrices only.
        </p>

        <h2>5. Conclusion</h2>

        <p>
            In this project we implemented four systems to make recommendations:
        </p>

        <ul>
            <li>Rank-based recommendation.</li>
            <li>User-user based collaborative filtering.</li>
            <li>Content-based collaborative filtering.</li>
            <li>Matrix Factorization.</li>
        </ul>

        <p>
            The combination of all these four systems could be used together
            to make recommendations to existing and to new users.
        </p>

        <p>
            To test the quality of the recommendations an online experiment could be performed.
            The users of the platform would be divided in two groups,
            one where the users receive no recommendation,
            and the other where the users receive indications
            from a combination of the recommendation systems implemented.
            After a reasonable period of time,
            an A/B test could be used to access the increase in the number of articles read.
            The metric to measure the success of the experiment
            could be the difference in the mean of the number of articles the user interacts with
            in each group, after the defined period of time.
            In this case,
            the null hypothesis is that the difference in the means is no greater than 0,
            and the alternative is that there is a difference in the means of both groups.
        </p>

        <h2>Extra: Content-Based Recommendations</h2>

        <p>
            Natural Language Processing (NLP) was used to extract relevant information
            about the description of the articles to subsequently build a content based recommender.
            The following steps were taken:
        </p>

        <ul>
            <li>Data cleaning:
                duplicate rows removed;
                links, non alphanumerics, punctuations, and stop words removed;
                stemming.
            </li>
            <li>
                Convert the description of the articles into sparse word-vectors,
                then apply dimensionality reduction techniques to find K topics:
                First the text of the description of each article was converted into word vectors
                using TFIDF (term frequency, inverse document frequency),
                then SVD (singular value decomposition) or NMF (non-negative matrix factorization)
                was applied to reduce the content of the articles into a few categories.
            </li>
        </ul>

        <p>
            After these steps,
            each article will be associated to a vector which contains weights
            that determines the connection to each of the K topics defined earlier.
            The similarity between any two article can be computed from this projection
            using cosine similarity.
        </p>

        <p>
            This is still an ongoing part of the project, and some steps are still missing:
        </p>

        <ul>
            <li>Apply different techniques to vectorize the text.</li>
            <li>Apply different dimensionality reduction techniques</li>
            <li>Evaluate the accuracy of the final models.</li>
        </ul>

        <p>
            However, the steps implemented so far
            can already give an idea of how to extract information from the items
            to create similar items recommendations.
            For more detail, please look at the Jupyter Notebook at Part IV.
        </p>


    </div>

</div>

</body>

<script>
    const histogram_figure = {{ histogram_articles_user | safe }}
    const data = histogram_figure['data']
    const layout = histogram_figure['layout']
    Plotly.newPlot("histogram_articles_user", data, layout)
</script>

<script>
    var top_10_articles_by_topic = JSON.parse('{{ top_10_articles_by_topic | tojson | safe }}')
</script>

<script type="module">

    import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js";
    import notebook from "https://api.observablehq.com/@d3/collapsible-tree.js?v=3";

    const runtime = new Runtime();
    const main = runtime.module(notebook, name => {
        if (name === "chart") {
            return new Inspector(document.querySelector("#collapsibleTree .observablehq-chart"));
        }
    });
    main.redefine("data", top_10_articles_by_topic);

</script>

<script>
    var similar_users = JSON.parse('{{ similar_users | tojson | safe }}')
</script>

<script type="module">

    import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js";
    import notebook from "https://api.observablehq.com/@d3/hierarchical-edge-bundling.js?v=3";

    const runtime = new Runtime();
    const main = runtime.module(notebook, name => {
        if (name === "chart") {
            return new Inspector(document.querySelector("#edgeBundling"));
        }
    });
    main.redefine("data", similar_users);
    main.redefine("colorin", "#2f4f4f")
</script>


</html>
